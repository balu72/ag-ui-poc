# ðŸ“¡ SSE (Server-Sent Events) Explained

## Great Question! How Do Stream Events Work with REST?

---

## ðŸŽ¯ Short Answer

**SSE streaming STARTS with a normal REST call, but the connection stays open!**

```
Traditional REST:
Client â†’ Request â†’ Server
Client â† Response â† Server
[Connection closes] âœ…

SSE (Our POC):
Client â†’ Request â†’ Server
Client â† Event 1 â† Server
Client â† Event 2 â† Server  [Connection stays open]
Client â† Event 3 â† Server
Client â† Event N â† Server
[Connection closes when stream ends] âœ…
```

---

## ðŸ“‹ Detailed Explanation

### **Step 1: Frontend Makes a REST Call**

```typescript
// This IS a normal REST call!
const response = await fetch("http://localhost:8000/chat", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    messages: [...],
    model: "mistral:latest"
  })
});
```

**This is HTTP POST** - Nothing special here!

---

### **Step 2: Server Responds with Special Headers**

Instead of sending a single JSON response and closing, the server responds with:

```python
return StreamingResponse(
    stream_ollama_response(...),
    media_type="text/event-stream",  # â† Special media type!
    headers={
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",   # â† Keep connection open!
        "X-Accel-Buffering": "no"
    }
)
```

**Key headers:**
- `Content-Type: text/event-stream` - Tells client this is SSE
- `Connection: keep-alive` - Don't close connection
- `Cache-Control: no-cache` - Don't cache events

---

### **Step 3: Connection Stays Open**

```
Normal REST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client                     Server
  â”‚                           â”‚
  â”œâ”€â”€â”€â”€ POST /chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
  â”‚                           â”‚
  â”‚â—€â”€â”€â”€â”€ {response} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                           â”‚
  X (closed)                  X
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SSE Stream:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client                     Server
  â”‚                           â”‚
  â”œâ”€â”€â”€â”€ POST /chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
  â”‚                           â”‚
  â”‚â—€â”€â”€â”€ data: event1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚â—€â”€â”€â”€ data: event2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚â—€â”€â”€â”€ data: event3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Connection stays open!
  â”‚â—€â”€â”€â”€ data: eventN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                           â”‚
  X (closed after stream)     X
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## ðŸ” In Our POC: The Complete Flow

### **Frontend Code:**

```typescript
// 1. Make REST call (POST request)
const response = await fetch("http://localhost:8000/chat", {
  method: "POST",
  body: JSON.stringify({...})
});

// 2. Get the response body as a stream
const reader = response.body?.getReader();

// 3. Read from the stream (this is the key!)
while (true) {
  const { done, value } = await reader.read();
  if (done) break;  // Stream ended, connection closes
  
  // Process the chunk
  const chunk = decoder.decode(value);
  console.log("Received:", chunk);
}
```

**What happens:**
1. `fetch()` sends HTTP POST (normal REST)
2. Server responds with `200 OK` immediately
3. Response body is a **ReadableStream** (not closed)
4. `reader.read()` waits for data from open connection
5. Server sends events whenever ready
6. Frontend receives and processes each event
7. When server closes stream, `done: true`

---

### **Backend Code:**

```python
@app.post("/chat")
async def chat(request: ChatRequest):
    # This is a normal REST endpoint!
    return StreamingResponse(
        stream_ollama_response(request.messages),
        media_type="text/event-stream"
    )

async def stream_ollama_response(messages):
    # Send event 1
    yield "data: {\"type\":\"start\"}\n\n"
    
    # Send event 2
    yield "data: {\"type\":\"text_message\", \"data\":{\"content\":\"Hello\"}}\n\n"
    
    # Send event 3
    yield "data: {\"type\":\"text_message\", \"data\":{\"content\":\" world\"}}\n\n"
    
    # Send event N
    yield "data: {\"type\":\"end\"}\n\n"
    
    # Function ends â†’ Stream closes â†’ Connection closes
```

**What happens:**
1. FastAPI endpoint receives POST (normal REST)
2. Returns `StreamingResponse` (keeps connection open)
3. Each `yield` sends data to client immediately
4. Function can take time between yields
5. When function ends, stream closes

---

## ðŸŽ¬ Timeline Example

**User types:** "Change color to light green"

```
T=0ms: Frontend sends POST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
POST http://localhost:8000/chat
Content-Type: application/json
Body: {"messages": [...]}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=10ms: Server responds with headers
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HTTP/1.1 200 OK
Content-Type: text/event-stream
Connection: keep-alive
Cache-Control: no-cache

[Body starts streaming - connection stays open]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=50ms: First event sent
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"type":"ui_control","data":{"action":"change_theme","color":"#90EE90"}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=100ms: Second event sent
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"type":"start","data":{"agent":"ollama"}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=200ms: Text starts streaming
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"type":"text_message","data":{"content":"Done"}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=220ms: More text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"type":"text_message","data":{"content":"!"}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=240ms: More text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"type":"text_message","data":{"content":" I've"}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

... continues ...

T=2000ms: Final event
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data: {"type":"end","data":{"status":"completed"}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T=2001ms: Stream ends, connection closes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Connection closed by server]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Total connection time:** ~2 seconds (all in one HTTP request!)

---

## ðŸ”‘ Key Points

### **1. It IS a REST Call**
```typescript
fetch("http://localhost:8000/chat", { method: "POST" })
```
- Standard HTTP POST request
- Normal REST endpoint
- Returns HTTP 200 OK

### **2. But Response is Streamed**
```typescript
Content-Type: text/event-stream
```
- Special content type
- Connection kept alive
- Multiple "chunks" sent

### **3. It's Still HTTP**
- Uses HTTP/1.1 protocol
- Same TCP connection
- No WebSocket needed
- Simpler than WebSocket

---

## ðŸ“Š Comparison

### **Traditional REST:**
```
Client: "Give me data"
Server: "Here's all the data" [closes]

Request:  1
Response: 1
Data:     All at once
```

### **SSE (Our POC):**
```
Client: "Give me data"
Server: "Here's part 1"
Server: "Here's part 2"
Server: "Here's part 3"
Server: "All done" [closes]

Request:  1
Response: Many (streamed)
Data:     Piece by piece
```

### **WebSocket (NOT used in our POC):**
```
Client: "Hello"
Server: "Hello back"
Client: "Data please"
Server: "Here's data"
Server: "More data"
Client: "Thanks"

Request:  Many
Response: Many
Data:     Bi-directional
```

---

## ðŸŽ¯ Why SSE for Our POC?

### **Advantages:**
âœ… **Simpler than WebSocket** - Just HTTP
âœ… **Automatic reconnection** - Browser handles it
âœ… **Text-based** - Easy to debug
âœ… **One direction** - Server â†’ Client (what we need)
âœ… **Built into HTTP** - No extra protocols

### **Perfect for:**
- Server â†’ Client streaming
- Real-time updates
- AI response streaming
- Progress updates
- Event notifications

### **NOT for:**
- Client â†’ Server streaming (use WebSocket)
- Binary data (use WebSocket)
- True bidirectional (use WebSocket)

---

## ðŸ’» Network Tab Evidence

**In Chrome DevTools â†’ Network tab:**

```
Name: chat
Status: 200
Type: eventsource          â† Special type!
Initiator: fetch
Size: (pending)            â† Data arrives over time
Time: 2.1s                 â† How long stream was open
```

**Click on the request:**

**Headers:**
```
Request URL: http://localhost:8000/chat
Request Method: POST        â† Normal REST!
Status Code: 200 OK

Response Headers:
  Content-Type: text/event-stream
  Connection: keep-alive
  Cache-Control: no-cache
```

**Response (Preview tab):**
```
data: {"type":"ui_control",...}

data: {"type":"start",...}

data: {"type":"text_message",...}

data: {"type":"text_message",...}

data: {"type":"end",...}
```

---

## ðŸ”§ Summary

### **The Answer:**

**SSE doesn't bypass REST - it extends it!**

1. **Frontend makes REST call** (POST /chat)
2. **Server returns 200 OK** immediately
3. **Connection stays open** (SSE headers)
4. **Server sends events** (multiple data chunks)
5. **Connection closes** when done

**It's ONE HTTP request** that lasts longer and sends multiple responses!

---

## ðŸ“ Analogy

**Traditional REST:**
```
You: "Can I have a coffee?"
Barista: "Here's your coffee" [done]
```

**SSE (Our POC):**
```
You: "Can I have a coffee?"
Barista: "I'm starting..."
Barista: "Grinding beans..."
Barista: "Brewing..."
Barista: "Adding milk..."
Barista: "Here's your coffee" [done]
```

**WebSocket:**
```
You: "Hi"
Barista: "Hi"
You: "Coffee please"
Barista: "Starting..."
Barista: "Done"
You: "Thanks"
Barista: "Welcome"
[Ongoing conversation]
```

---

## ðŸŽ¯ Final Answer

**Question:** "How is stream events from backend bypassing normal REST call?"

**Answer:** **It's NOT bypassing REST!**

- âœ… Starts with normal HTTP POST
- âœ… Uses REST endpoint (/chat)
- âœ… Returns HTTP 200 OK
- âœ… But connection stays open (SSE)
- âœ… Server sends multiple events
- âœ… Finally connection closes

**It's a REST call that streams the response!** ðŸš€

---

## â“ FAQ: Common Questions

### **Q: Do ALL REST endpoints send streams?**

**A: No! Only endpoints you design to stream.**

In our POC, we have **3 endpoints:**

#### **1. Regular REST Endpoint (Single Response)**

```python
@app.get("/")
async def root():
    return {
        "service": "AG-UI POC",
        "status": "running"
    }
    # Returns once â†’ Connection closes âœ…
```

**Browser receives:**
```json
{"service": "AG-UI POC", "status": "running"}
```
Connection closes immediately.

---

#### **2. Another Regular REST Endpoint**

```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "ollama": "connected"
    }
    # Returns once â†’ Connection closes âœ…
```

**Browser receives:**
```json
{"status": "healthy", "ollama": "connected"}
```
Connection closes immediately.

---

#### **3. STREAMING REST Endpoint** (The Special One!)

```python
@app.post("/chat")
async def chat(request: ChatRequest):
    return StreamingResponse(        # â† This makes it stream!
        stream_ollama_response(...),
        media_type="text/event-stream"  # â† Special type!
    )
    # Streams multiple events â†’ Closes when generator ends âœ…
```

**Browser receives (over time):**
```
data: {"type":"start"}

data: {"type":"text_message","data":{"content":"Hello"}}

data: {"type":"text_message","data":{"content":" world"}}

data: {"type":"end"}
```
Connection stays open, closes after last event.

---

### **Key Difference**

**Normal Endpoint:**
```python
@app.get("/api/data")
async def get_data():
    return {"result": "data"}  # â† Single return
```

**Streaming Endpoint:**
```python
@app.get("/api/stream")
async def stream_data():
    async def generate():
        yield "event 1"
        yield "event 2"
        yield "event 3"
    
    return StreamingResponse(generate())  # â† Generator function
```

---

### **In Our POC - Endpoint Comparison**

| Endpoint | Type | Response | Connection |
|----------|------|----------|------------|
| `GET /` | Normal REST | Single JSON | Closes immediately |
| `GET /health` | Normal REST | Single JSON | Closes immediately |
| `POST /chat` | **Streaming REST** | **Multiple events** | **Stays open** |

**Only `/chat` uses streaming!** The other endpoints are traditional REST.

---

### **Summary**

**Question:** "So each REST endpoint can send stream of events here?"

**Answer:**
- âŒ **Not by default**
- âœ… **Only if you use `StreamingResponse`** in FastAPI
- âœ… **In our POC: Only `/chat` endpoint streams**
- âœ… **Other endpoints (`/`, `/health`) are normal REST**

**You choose which endpoints stream when you build them!**

**It's like choosing between:**
- ðŸ• **Regular delivery:** "Here's your pizza" [done]
- ðŸ“¦ **Progress tracking:** "Order received" â†’ "Preparing" â†’ "Out for delivery" â†’ "Delivered" [multiple updates]

Same service, different delivery methods!

---

## â“ FAQ: How Does Frontend Handle Different Responses?

### **Q: Does frontend code handle regular REST vs streaming differently?**

**A: YES! Completely different code patterns.**

---

## ðŸ”„ Frontend: Two Different Patterns

### **Pattern 1: Regular REST Response**

```typescript
// For endpoints like GET /, GET /health
async function callRegularAPI() {
  const response = await fetch("http://localhost:8000/health");
  
  // Single await - gets complete response
  const data = await response.json();  // â† One call, complete data
  
  console.log(data);  // {"status": "healthy", ...}
  // Done! Connection already closed.
}
```

**Key points:**
- `await response.json()` - Gets complete response at once
- Single data object returned
- Connection closes immediately after response
- Simple and straightforward

---

### **Pattern 2: Streaming REST Response**

```typescript
// For endpoints like POST /chat
async function callStreamingAPI() {
  const response = await fetch("http://localhost:8000/chat", {
    method: "POST",
    body: JSON.stringify({...})
  });
  
  // Get stream reader instead of json()
  const reader = response.body?.getReader();  // â† Get reader
  const decoder = new TextDecoder();
  
  // Loop to read chunks as they arrive
  while (true) {  // â† Keep reading
    const { done, value } = await reader.read();
    if (done) break;  // Stream ended
    
    const chunk = decoder.decode(value);
    console.log("Chunk:", chunk);  // Process each chunk
  }
  // Now connection closes
}
```

**Key points:**
- `response.body.getReader()` - Gets stream reader
- Loop continuously reads chunks
- Each chunk processed as it arrives
- Connection stays open during loop
- Breaks when `done: true`

---

## ðŸ“Š Side-by-Side Comparison

### **Regular REST (GET /health)**

```typescript
// SIMPLE - One request, one response
const response = await fetch("/health");
const data = await response.json();
console.log(data.status);  // Done!
```

**What frontend does:**
1. Send request
2. Wait for complete response
3. Parse JSON once
4. Use the data
5. âœ… Done

---

### **Streaming REST (POST /chat)**

```typescript
// COMPLEX - One request, multiple responses
const response = await fetch("/chat", {...});
const reader = response.body.getReader();

while (true) {
  const {done, value} = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  // Process chunk
  parseAndHandle(chunk);
}
```

**What frontend does:**
1. Send request
2. Get stream reader
3. Loop waiting for chunks
4. Process each chunk as arrives
5. Parse each chunk individually
6. Update UI for each chunk
7. âœ… Done when stream ends

---

## ðŸŽ¯ In Our POC: Actual Code

### **Regular Endpoint (Not Used for Chat)**

```typescript
// If we had a simple endpoint
async function checkHealth() {
  const response = await fetch("http://localhost:8000/health");
  const data = await response.json();
  
  // All data at once
  console.log(data);
  // {"status": "healthy", "ollama": "connected"}
}
```

---

### **Streaming Endpoint (Used for Chat)**

```typescript
// Our actual chat code
async function sendMessage() {
  const response = await fetch("http://localhost:8000/chat", {
    method: "POST",
    body: JSON.stringify({messages, model})
  });

  // Special handling for streaming
  const reader = response.body?.getReader();
  const decoder = new TextDecoder();
  let assistantMessage = "";

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split("\n");
    
    for (const line of lines) {
      if (line.startsWith("data: ")) {
        const event = JSON.parse(line.substring(6));
        
        // Handle different event types
        if (event.type === "text_message") {
          assistantMessage += event.data.content;
          updateUI(assistantMessage);  // Update UI for each chunk!
        }
        else if (event.type === "ui_control") {
          applyUIChange(event.data);  // Change color/add button!
        }
      }
    }
  }
}
```

---

## ðŸ”‘ Key Differences Table

| Aspect | Regular REST | Streaming REST |
|--------|--------------|----------------|
| **Parse method** | `response.json()` | `response.body.getReader()` |
| **Waiting** | Single `await` | Loop with multiple `await` |
| **Data arrival** | All at once | Piece by piece |
| **Processing** | Once | Multiple times |
| **UI updates** | Once at end | Multiple times during |
| **Code complexity** | Simple | More complex |
| **Connection** | Closes immediately | Stays open |

---

## ðŸ’¡ Why the Difference?

### **Regular REST:**
```typescript
response.json()  // Browser handles everything
                 // Returns complete parsed object
                 // Simple!
```

### **Streaming REST:**
```typescript
response.body.getReader()  // You handle chunks
                           // Parse each chunk yourself
                           // Process as you go
                           // More control, more code
```

---

## ðŸŽ¬ Real Example: Our POC

### **Hypothetical: If we called /health**

```typescript
// Simple code (but we don't use this endpoint in chat)
const health = await fetch("/health").then(r => r.json());
console.log(health.status);
```

**Output:**
```
{status: "healthy", ollama: "connected"}
```
One line logged, done.

---

### **Actual: When we call /chat**

```typescript
// Complex streaming code (what we actually use)
const response = await fetch("/chat", {...});
const reader = response.body.getReader();

while (true) {
  const chunk = await reader.read();
  // ... parse and process ...
}
```

**Output over time:**
```
Chunk 1: data: {"type":"ui_control",...}
Chunk 2: data: {"type":"start",...}
Chunk 3: data: {"type":"text_message",...}
Chunk 4: data: {"type":"text_message",...}
...
Chunk N: data: {"type":"end",...}
```
Multiple lines logged as they arrive.

---

## ðŸŽ¯ Summary

**Question:** "This means the receiving front code needs to handle response from REST endpoint differently. How?"

**Answer:**

### **For Regular REST:**
```typescript
const data = await response.json();
// Simple! All data at once.
```

### **For Streaming REST:**
```typescript
const reader = response.body.getReader();
while (true) {
  const chunk = await reader.read();
  // Complex! Process each chunk.
}
```

**You MUST use the reader pattern for streaming endpoints!**

**Can't use `.json()` on a stream** - it would try to wait for the complete response (which never comes as a single chunk).

**Must use `.getReader()` and loop** - this is designed for reading data over time.

---

## ðŸ”§ Quick Reference

### **When you see this in backend:**
```python
return {"data": "value"}  # Regular
```

### **Frontend uses:**
```typescript
await response.json()
```

---

### **When you see this in backend:**
```python
return StreamingResponse(generator())  # Streaming
```

### **Frontend MUST use:**
```typescript
response.body.getReader() + while loop
```

**Different backend â†’ Different frontend pattern!** ðŸš€
