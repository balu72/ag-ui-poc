# ðŸ” Debugging Guide for AG-UI POC

This guide helps you troubleshoot and understand the logging in your AG-UI POC application.

## Quick Testing

Run the test script to verify everything is working:
```bash
./test-backend.sh
```

This will:
1. Check if backend is running
2. Verify Ollama connection
3. Send a test message
4. Show you the response

## Understanding Backend Logs

When the backend is running, you'll see detailed logs in the terminal. Here's what each emoji means:

### ðŸŒ HTTP Endpoints
- `ðŸŒ [CHAT]` - Direct `/chat` endpoint received a request
- `ðŸ¤– [COPILOTKIT]` - CopilotKit endpoint `/v1/copilotkit` received a request

### ðŸ”µ Streaming Process
- `ðŸ”µ [STREAM]` - Stream initialization
- `ðŸ”µ [OLLAMA]` - Communication with Ollama

### ðŸ“¤ Event Emission
- `ðŸ“¤ [EVENT]` - AG-UI protocol events being sent to frontend
  - START event - Agent starting
  - TEXT_MESSAGE events - Streaming chunks
  - RESULT event - Complete response
  - END event - Agent finished

### ðŸŸ¢ Success
- `ðŸŸ¢ [OLLAMA]` - Ollama stream started successfully

### ðŸ“ Data Flow
- `ðŸ“ [CHUNK]` - Individual chunks from Ollama
- `ðŸ“¨` - Individual messages in the conversation

### âœ… Completion
- `âœ… [STREAM]` - Stream completed successfully

### âŒ Errors
- `âŒ [ERROR]` - Something went wrong
- `âŒ [CHAT]` or `âŒ [COPILOTKIT]` - Endpoint error

## Common Issues and Solutions

### 1. "No response" from Frontend

**Check Backend Logs for:**
```
ðŸ¤– [COPILOTKIT] Received /v1/copilotkit request
```

**If you DON'T see this:**
- âœ… Check CORS settings in `backend/main.py`
- âœ… Verify frontend is connecting to `http://localhost:8000`
- âœ… Open browser DevTools â†’ Network tab â†’ Check for failed requests

**If you DO see it:**
- Look for the next logs:
  ```
  ðŸ”µ [STREAM] Starting stream
  ðŸ”µ [OLLAMA] Calling Ollama
  ```

### 2. Ollama Connection Issues

**Look for:**
```
âŒ [ERROR] Exception in stream: ...
```

**Common errors:**
- `Connection refused` â†’ Ollama is not running
  - **Fix:** Run `ollama serve` in a separate terminal
  
- `model 'mistral:latest' not found` â†’ Model not downloaded
  - **Fix:** Run `ollama pull mistral:latest`

- `timeout` â†’ Ollama is slow or overloaded
  - **Fix:** Wait a moment and try again

### 3. Empty or Incomplete Responses

**Check for:**
```
ðŸ“ [CHUNK X] Received: ...
```

**If chunks are being received but frontend shows nothing:**
- âœ… Check browser DevTools â†’ Console for errors
- âœ… Verify CopilotKit license key is valid
- âœ… Check SSE connection in Network tab (type: `eventsource`)

### 4. Frontend Not Loading

**Browser Console Errors:**
- `Cannot find module '@copilotkit/react-core'`
  - **Fix:** Run `cd frontend && npm install`

- `publicLicenseKey` errors
  - **Fix:** Verify the license key in `App.tsx`

- CORS errors
  - **Fix:** Check backend CORS settings allow `http://localhost:5173`

## Testing Individual Components

### Test Backend Only
```bash
# Terminal 1: Start backend
./start-backend.sh

# Terminal 2: Test with curl
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello"}]}'
```

**Expected logs:**
```
ðŸŒ [CHAT] Received /chat request
ðŸ”µ [STREAM] Starting stream
ðŸ”µ [OLLAMA] Calling Ollama
ðŸŸ¢ [OLLAMA] Stream started successfully
ðŸ“ [CHUNK 1] Received: Hello...
âœ… [STREAM] Received X chunks
```

### Test Ollama Directly
```bash
curl http://localhost:11434/api/tags
```

Should return list of installed models.

### Test Frontend-Backend Communication

**In Browser DevTools (Network tab):**
1. Look for request to `http://localhost:8000/v1/copilotkit`
2. Check Status: should be `200 OK`
3. Check Type: should be `eventsource` or `text/event-stream`
4. Look at Preview tab: should show SSE events

## Detailed Log Example

Here's what a successful request looks like:

```
============================================================
ðŸ¤– [COPILOTKIT] Received /v1/copilotkit request
ðŸ¤– [COPILOTKIT] Request keys: ['messages', 'model']
ðŸ¤– [COPILOTKIT] Full request: {
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "model": "mistral:latest"
}
============================================================

ðŸ¤– [COPILOTKIT] Extracted 1 messages
ðŸ¤– [COPILOTKIT] Model: mistral:latest
ðŸ¤– [COPILOTKIT] Converted to 1 message objects
  ðŸ“¨ Message 1: [user] Hello

ðŸ”µ [STREAM] Starting stream for model: mistral:latest
ðŸ”µ [STREAM] Messages count: 1
ðŸ“¤ [EVENT] Sending START event: data: {"type": "start", ...

ðŸ”µ [OLLAMA] Calling Ollama with 1 messages
ðŸ”µ [OLLAMA] Last message: {'role': 'user', 'content': 'Hello'}
ðŸŸ¢ [OLLAMA] Stream started successfully

ðŸ“ [CHUNK 1] Received: Hello...
ðŸ“ [CHUNK 2] Received: !...
ðŸ“ [CHUNK 3] Received:  How...

âœ… [STREAM] Received 25 chunks, total length: 123
ðŸ“¤ [EVENT] Sending RESULT event with 123 chars
ðŸ“¤ [EVENT] Sending END event
```

## Getting Help

If you're still stuck:

1. **Capture the logs:**
   ```bash
   ./start-backend.sh 2>&1 | tee backend.log
   ```

2. **Check the logs for:**
   - Any âŒ error markers
   - The last successful step before failure
   - Full error messages and stack traces

3. **Verify the basics:**
   - [ ] Ollama is running (`ollama serve`)
   - [ ] Model is installed (`ollama list | grep mistral`)
   - [ ] Backend is running (visit http://localhost:8000)
   - [ ] Frontend is running (visit http://localhost:5173)
   - [ ] No port conflicts

4. **Common Port Issues:**
   ```bash
   # Check if ports are in use
   lsof -i :8000  # Backend
   lsof -i :5173  # Frontend
   lsof -i :11434 # Ollama
   ```

## Environment Check

Run this to verify your environment:
```bash
echo "=== Environment Check ==="
echo "Python: $(python --version 2>&1)"
echo "Node: $(node --version 2>&1)"
echo "npm: $(npm --version 2>&1)"
echo ""
echo "=== Ollama Status ==="
curl -s http://localhost:11434/api/tags | head -20
echo ""
echo "=== Backend Status ==="
curl -s http://localhost:8000/ | head -5
```

## Still Need Help?

Review the README files:
- Root: `README.md` - General overview
- Backend: `backend/README.md` - Backend specific
- Frontend: `frontend/README.md` - Frontend specific
